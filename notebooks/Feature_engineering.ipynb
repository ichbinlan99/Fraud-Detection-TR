{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Tranformation and Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From EDA, we derived some actionable insights and would love to further transform and create features for further research. 3 most important steps that I want to point out are:\n",
    "\n",
    "1. generating the job normalisation gazetteer\n",
    "\n",
    "2. create customer spending behaviour on a rolling window base\n",
    "\n",
    "3. calculate merchant risk factor on a rolling window base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Feature Transformation and Engineering](#feature-transformation-and-engineering)\n",
    "    1. [Feature Transformation](#feature-transformation)\n",
    "        1. [Encode Gender](#encode-gender)\n",
    "        2. [Encode Job](#encode-job)\n",
    "        3. [Encode Merchant Category](#encode-merchant-category)\n",
    "    2. [Feature Engineering](#feature-engineering)\n",
    "        1. [Age Group](#age-group)\n",
    "        2. [Customer Spending Behaviour](#customer-spending-behaviour)\n",
    "        3. [City Size](#city-size)\n",
    "        4. [Merchant Risk Factor](#merchant-risk-factor)\n",
    "2. [Save Data](#save-data)\n",
    "3. [Outlook](#outlook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "from geopy.distance import geodesic\n",
    "from matplotlib import pyplot as plt\n",
    "from summarytools import dfSummary\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))\n",
    "\n",
    "from utils.plots import FraudMap\n",
    "from features.feature_engineering import generic_customer_spending_behaviour, general_customer_bahaviour, get_merchant_risk_rolling_window\n",
    "from features.feature_transformation import encode, categorize_jobs\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load transaction data\n",
      "CPU times: user 5.78 s, sys: 825 ms, total: 6.61 s\n",
      "Wall time: 6.88 s\n",
      "1296675 transaction data loaded, containing 7506 fraudulent transactions\n"
     ]
    }
   ],
   "source": [
    "print(\"Load transaction data\")\n",
    "%time df = pd.read_csv(\"../data/raw/tr_fincrime_train.csv\")\n",
    "print(\"{0} transaction data loaded, containing {1} fraudulent transactions\".format(len(df),df['is_fraud'].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section mainly includes datetime transformation and categorical feature encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'trans_date_trans_time' column to datetime if not already done\n",
    "df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'])\n",
    "# Extract the date part from the 'trans_date_trans_time' column\n",
    "df['trans_date'] = df['trans_date_trans_time'].dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the gender column\n",
    "df_enc_gender, _ = encode(df, \"gender\", \"gender\", encoding = \"onehot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    709863\n",
       "1    586812\n",
       "Name: gender_M, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_enc_gender['gender_M'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job column contains ambuigious entries of 494 different values with most of them being similar to each other (\"engineer, water\" and \"engineer, operation\"). We start by normalising these jobs into a pre-defined category. With the help of Gemini 1.5 pro, I divide the jobs into 11 categories:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Healthcare & Medical\n",
    "\n",
    "2. Engineering & Technology\n",
    "\n",
    "3. Finance, Banking & Insurance\n",
    "\n",
    "4. Education & Research\n",
    "\n",
    "5. Creative Arts, Design & Media\n",
    "\n",
    "6. Legal & Public Sector\n",
    "\n",
    "7. Business, Management & Consultancy\n",
    "\n",
    "8. Science & Research\n",
    "\n",
    "9. Logistics, Transport & Supply Chain\n",
    "\n",
    "10. Construction & Property\n",
    "\n",
    "11. Hospitality, Tourism & Leisure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load job normalisation gazetteer\n",
    "with open('../jobs_by_category.json', 'r') as f:\n",
    "    job_categories = json.load(f)\n",
    "# normalize and categorize the job column \n",
    "df_cat_job = categorize_jobs(df_enc_gender, \"job\", job_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the job category\n",
    "df_enc_job, job_encoder = encode(df_cat_job, \"job_category\", \"job_encoded\", encoding=\"ordinal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Business, Management & Consultancy': 0, 'Construction & Property': 1, 'Creative Arts, Design & Media': 2, 'Education & Research': 3, 'Engineering & Technology': 4, 'Finance, Banking & Insurance': 5, 'Healthcare & Medical': 6, 'Hospitality, Tourism & Leisure': 7, 'Legal & Public Sector': 8, 'Logistics, Transport & Supply Chain': 9, 'Science & Research': 10}\n"
     ]
    }
   ],
   "source": [
    "mapping = {category: int(code) for category, code in zip(job_encoder.categories_[0], range(len(job_encoder.categories_[0])))}\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the encoder\n",
    "with open('../saved_model/encoders/job_encoder.pkl', 'wb') as file:\n",
    "    pickle.dump(job_encoder, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, you might wonder, what if we invented a new job like \"gardening engineer\" and is now logged in the data storage for testing? \n",
    "\n",
    "The test data for the job category are handled as follows: we first compare the job in test data to the [job normalisation gazetter](https://github.com/ichbinlan99/Fraud-Detection-TR/blob/eda/jobs_by_category.json) to normalise existing jobs. Then, we look at the non-matched jobs and use fuzzy match to match it with the existing jobs. If the similarity score is too low, we add a new category \"Other\" and use ordinal encoder to handle the new added category.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Merchant Category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the merchant category only has 14 categories in the training set, we handle it similary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enc_cat, merchant_cat_encoder = encode(df_enc_job, \"category\", \"category_encoded\", encoding=\"ordinal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entertainment': 0, 'food_dining': 1, 'gas_transport': 2, 'grocery_net': 3, 'grocery_pos': 4, 'health_fitness': 5, 'home': 6, 'kids_pets': 7, 'misc_net': 8, 'misc_pos': 9, 'personal_care': 10, 'shopping_net': 11, 'shopping_pos': 12, 'travel': 13}\n"
     ]
    }
   ],
   "source": [
    "# Create the mapping\n",
    "mapping = {category: int(code) for category, code in zip(merchant_cat_encoder.categories_[0], range(len(merchant_cat_encoder.categories_[0])))}\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the encoder\n",
    "with open('../saved_model/encoders/merchant_cat_encoder.pkl', 'wb') as file:\n",
    "    pickle.dump(merchant_cat_encoder, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Age Group "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the age of the credicard holder when the transaction was made since the trascation history has a duration of 536 days and divide them into age groups. The choice of group boundary and bandwidth could be decided in several ways. Either be looking at the quantile of the age distribution or heuristically from domain knowledge or look at the 'risk' of being frauded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enc_cat[\"dob\"] = pd.to_datetime(df_enc_cat[\"dob\"], errors=\"coerce\")\n",
    "df_enc_cat[\"trans_date_trans_time\"] = pd.to_datetime(\n",
    "    df_enc_cat[\"trans_date_trans_time\"], errors=\"coerce\"\n",
    ")\n",
    "df_enc_cat[\"age\"] = (df_enc_cat[\"trans_date_trans_time\"] - df_enc_cat[\"dob\"]).dt.days // 365"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average retire age in the US is around 63-65 (65+) and considering average poeple leave university around 25. More than 82% of adults(25-54) had a credit card as of 2023. Financially, most people hit their career peak in their late 40s, 50s. Considering all these factors combined with the plot from EDA, I decided to group the card holders in 4 age groups: Under 25, 25-45, 45-65, 65+.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"../out/age.png\" width=500>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define age bins based on observed patterns\n",
    "# another level on larger granular level would be 0-25, 25-50, 50+\n",
    "bins = [0, 25, 45, 65, 100]\n",
    "labels = [\"Under 25\", \"25-45\", \"45-65\", \"65+\"]\n",
    "true_lables = [1, 0, 2, 3]  # 0 has the lowest fraud rate (risk), 1 has the highest\n",
    "df_enc_cat[\"age_group\"] = pd.cut(df_enc_cat[\"age\"], bins=bins, labels=true_lables)\n",
    "df_enc_cat[\"age_group\"] = df_enc_cat[\"age_group\"].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Customer Spending behaviour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fraudsters typically try to make the most out of the stolen card before the fraud is detected. Aside from making several purchases in quick period of time, the fraudsters also try to get high transactions suggesting that a high amount of transaction than average history spending habbit of the crad holder could be suspicious. On the other hand, individuals may exhibit patterned bahaviouss While this could be a good indicator for a certain transaction being flagged as fraudulent, we might also want to capture a global pattern to combine with. Thus, we quantify the customer spending behaviour in the below 2 aspects:\n",
    "\n",
    "1. Generic spending bahaviour (indicating individual behaviours):\n",
    "\n",
    "- `avg_amount_{window_length}_days`: Rolling average transaction amount for the past {window_length} days.\n",
    "- `count_amount_{window_length}_days`: Number of transactions in the past {window_length} days.\n",
    "- `amount_over_average_{window_length}_days`: Ratio of transaction amount to the rolling average amount.\n",
    "- `inter_transaction_time_{window_length}_days`: Time (in seconds) since the last transaction.\n",
    "- `card_count_sparsity`: Average transaction counts per card per day.\n",
    "- `merchant_count_sparsity`: Average transaction counts per merchant per day.\n",
    "- `card_amount_sparsity`: Average transaction amounts per card per day.\n",
    "- `merchant_amount_sparsity`: Average transaction amounts per merchant per day.\n",
    "\n",
    "However, as the window size serves as a hypter-paramter and can be tuned later, here we generate a list of size of [1, 7, 30, 90, 180] days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"../out/customer_spending_behaviour.png\" width=1000>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, geographical discrepancies (Unusually far distance between cardholder and merchant address): When a cardholder's address and the merchant’s geographical location are far apart, this could potentially signal that the transaction is fraudulent. This is particularly relevant for Card Not Present (CNP) fraud, where fraudsters often use stolen card details to make purchases in distant locations or countries. Or if the cardholder has typically made purchases in one region and suddenly makes a large purchase in a city far away, it could indicate that the card details have been stolen and used without authorization. The geodesic distances are calculated using the [GeoPy](https://geopy.readthedocs.io/en/stable/) library. We add\n",
    "\n",
    "- `avg_distance_{window_length}_days`: Rolling average distance for the past {window_length} days.\n",
    "- `distance_over_avg_{window_length}_days`: Ratio of transaction distance to the rolling average distance.\n",
    "\n",
    "to the generic customer spending behaviour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"../out/geodesic.png\" width=180>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(row):\n",
    "    cust_location = (row[\"lat\"], row[\"long\"])\n",
    "    merch_location = (row[\"merch_lat\"], row[\"merch_long\"])\n",
    "    return geodesic(cust_location, merch_location).miles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enc_cat[\"distance\"] = df_enc_cat.apply(calculate_distance, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dist = generic_customer_spending_behaviour(df_enc_cat, window_lengths=[1, 7, 30, 90, 180]) #define time windows in 1, 7, 30, 90, 180 days\n",
    "df_dist = general_customer_bahaviour(df_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0010983665420605, 1.0105628470572936)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud_transactions, non_fraud_transactions = df_dist[df_dist[\"is_fraud\"] == 1], df_dist[df_dist[\"is_fraud\"] == 0]\n",
    "non_fraud_transactions['distance_over_avg_180_days'].mean(), fraud_transactions['distance_over_avg_180_days'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. General spending behaviour (indicating global patterns for the entire population):\n",
    "- `transaction_hour`: Hour of the transaction.\n",
    "- `transaction_day_of_week`: Day of the week of the transaction (0=Monday, 6=Sunday).\n",
    "- `is_holiday`: 1 if the transaction occurred on a US holiday, 0 otherwise.\n",
    "- `is_weekend`: 1 if the transaction occurred on a weekend, 0 otherwise.\n",
    "- `trans_date`: Date of the transaction.\n",
    "- `daily_trans_count`: Number of transactions for the given credit card on that day.\n",
    "\n",
    "Late-Night or Off-Hours Transactions: Transactions made outside typical shopping hours, such as late at night or during weekends or holidays when fraud is less likely to be detected, may indicate that a fraud is taking place, particularly if they are not in line with the cardholder’s usual behavior. In Addition, multiple transactions in a short period (e.g., multiple small purchases in one hour) may also indicates a fraudster testing or running up charges on a stolen card."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From EDA we noticed a huge discrepancy between the transaction ratio and fraud ratio on an hourly level thus we add a risk feature to indicate if the hour of the transaction lies in a high risk hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add a column to indicate if the transaction is in a high risk hour\n",
    "df_dist[\"is_high_risk_hour\"] = df_dist[\"transaction_hour\"].apply(\n",
    "    lambda x: 2 if x in [22, 23] else (1 if x in [0, 1, 2, 3] else 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. City Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Major Cities:** Often considered to be populations over 500,000 or even 1 million. Some might even set the bar higher, considering only the very largest metropolitan areas like New York, Los Angeles, and Chicago as truly \"big.\" Since the dataset dosen't contain much transaction history over cities with this much large of population, I took the margin at 150k.\n",
    "\n",
    "**Mid-Sized Cities**: This is where there's a lot of variation. A common range is between 100,000 and 500,000. Some definitions might go as low as 50,000 or as high as approaching 1 million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bins and labels for city size\n",
    "city_size_bins = [0, df_dist['city_pop'].quantile(0.85), df_dist['city_pop'].quantile(0.99) ,float('inf')]\n",
    "city_size_labels = ['Small', 'Medium', 'Large']\n",
    "city_size_labels_true = [0,1,2]\n",
    "\n",
    "# Create the city_size column\n",
    "df_dist['city_size'] = pd.cut(df_dist['city_pop'], bins=city_size_bins, labels=city_size_labels_true)\n",
    "df_dist['city_size'] = df_dist['city_size'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88735.0, 1577385.0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dist['city_pop'].quantile(0.85), df_dist['city_pop'].quantile(0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Merchat Risk Factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "The main goal will be to extract a risk score that assesses the exposure of a given merchant name to fraudulent transactions. The risk score will be defined as the average number of fraudulent transactions that occurred on a merchant name over a time window. The time windows will not directly precede a given transaction. Instead, they will be shifted back by a delay period. The delay period accounts for the fact that, in practice, the fraudulent transactions are only discovered after a fraud investigation or a customer complaint. Hence, the fraudulent labels, which are needed to compute the risk score, are only available after this delay period. To a first approximation, this delay period will be set to one week. Let us perform the computation of the risk scores by defining a get_merchant_risk_rolling_window function. The function takes as inputs the DataFrame of transactions for a given merchant name, the delay period, and a list of window sizes. In the first stage, the number of transactions and fraudulent transactions are computed for the delay period. In the second stage, the number of transactions and fraudulent transactions are computed for each window size plus the delay period. The number of transactions and fraudulent transactions that occurred for a given window size, shifted back by the delay period, is then obtained by simply computing the differences of the quantities obtained for the delay period and the window size plus delay period. The risk score is finally obtained by computing the proportion of fraudulent transactions for each window size (or 0 if no transaction occurred for the given window). Additionally, to the risk score, the function also returns the number of transactions for each window size.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"../out/merchant_risk_factor.png\" width=1000>\n",
    "</p>\n",
    "\n",
    "The risk score $R$ is computed as:\n",
    "$$\n",
    "R = \n",
    "\\begin{cases} \n",
    "\\frac{\\text{Number of fraudulent transactions in the window}}{\\text{Total transactions in the window}}, & \\text{if Total transactions > 0} \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- **Window size**: The time period for which transactions are considered (e.g., 1 week, 1 month).\n",
    "- **Delay period**: The time shift to account for the delay in fraud detection (e.g., 1 week).\n",
    "- **Total transactions in the window**: The number of transactions during the delay period + window size, minus the transactions during the delay period.\n",
    "- **Number of fraudulent transactions in the window**: The count of fraudulent transactions during the delay period + window size, minus those during the delay period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merch_risk = get_merchant_risk_rolling_window(transactions = df_dist, delay_period=7, window_size=[1, 7, 30, 90, 180])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.005483918793180499, 0.005806033249438283, 0.006133785097165681)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merch_risk['merchant_risk_1_day_window'].mean(), df_merch_risk['merchant_risk_30_day_window'].mean(), df_merch_risk['merchant_risk_180_day_window'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a quick overview of correlations between the variables in numerical values.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"../out/corr_map.png\" width=1000>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1296675 entries, 0 to 1296674\n",
      "Data columns (total 86 columns):\n",
      " #   Column                             Non-Null Count    Dtype         \n",
      "---  ------                             --------------    -----         \n",
      " 0   Unnamed: 0                         1296675 non-null  int64         \n",
      " 1   trans_date_trans_time              1296675 non-null  datetime64[ns]\n",
      " 2   cc_num                             1296675 non-null  int64         \n",
      " 3   merchant                           1296675 non-null  object        \n",
      " 4   category                           1296675 non-null  object        \n",
      " 5   amt                                1296675 non-null  float64       \n",
      " 6   first                              1296675 non-null  object        \n",
      " 7   last                               1296675 non-null  object        \n",
      " 8   street                             1296675 non-null  object        \n",
      " 9   city                               1296675 non-null  object        \n",
      " 10  state                              1296675 non-null  object        \n",
      " 11  zip                                1296675 non-null  int64         \n",
      " 12  lat                                1296675 non-null  float64       \n",
      " 13  long                               1296675 non-null  float64       \n",
      " 14  city_pop                           1296675 non-null  int64         \n",
      " 15  job                                1296675 non-null  object        \n",
      " 16  dob                                1296675 non-null  datetime64[ns]\n",
      " 17  trans_num                          1296675 non-null  object        \n",
      " 18  unix_time                          1296675 non-null  int64         \n",
      " 19  merch_lat                          1296675 non-null  float64       \n",
      " 20  merch_long                         1296675 non-null  float64       \n",
      " 21  is_fraud                           1296675 non-null  int64         \n",
      " 22  trans_date                         1296675 non-null  object        \n",
      " 23  gender_M                           1296675 non-null  uint8         \n",
      " 24  job_category                       1296675 non-null  object        \n",
      " 25  job_encoded                        1296675 non-null  float64       \n",
      " 26  category_encoded                   1296675 non-null  float64       \n",
      " 27  age                                1296675 non-null  int64         \n",
      " 28  age_group                          1296675 non-null  int64         \n",
      " 29  distance                           1296675 non-null  float64       \n",
      " 30  avg_distance_1_days                1296675 non-null  float64       \n",
      " 31  distance_over_avg_1_days           1296675 non-null  float64       \n",
      " 32  sum_amount_1_days                  1296675 non-null  float64       \n",
      " 33  count_amount_1_days                1296675 non-null  float64       \n",
      " 34  avg_amount_1_days                  1296675 non-null  float64       \n",
      " 35  amount_over_average_1_days         1296675 non-null  float64       \n",
      " 36  inter_transaction_time_1_days      1296675 non-null  float64       \n",
      " 37  card_count_sparsity                1296675 non-null  float64       \n",
      " 38  merchant_count_sparsity            1296675 non-null  float64       \n",
      " 39  card_amount_sparsity               1296675 non-null  float64       \n",
      " 40  merchant_amount_sparsity           1296675 non-null  float64       \n",
      " 41  avg_distance_7_days                1296675 non-null  float64       \n",
      " 42  distance_over_avg_7_days           1296675 non-null  float64       \n",
      " 43  sum_amount_7_days                  1296675 non-null  float64       \n",
      " 44  count_amount_7_days                1296675 non-null  float64       \n",
      " 45  avg_amount_7_days                  1296675 non-null  float64       \n",
      " 46  amount_over_average_7_days         1296675 non-null  float64       \n",
      " 47  inter_transaction_time_7_days      1296675 non-null  float64       \n",
      " 48  avg_distance_30_days               1296675 non-null  float64       \n",
      " 49  distance_over_avg_30_days          1296675 non-null  float64       \n",
      " 50  sum_amount_30_days                 1296675 non-null  float64       \n",
      " 51  count_amount_30_days               1296675 non-null  float64       \n",
      " 52  avg_amount_30_days                 1296675 non-null  float64       \n",
      " 53  amount_over_average_30_days        1296675 non-null  float64       \n",
      " 54  inter_transaction_time_30_days     1296675 non-null  float64       \n",
      " 55  avg_distance_90_days               1296675 non-null  float64       \n",
      " 56  distance_over_avg_90_days          1296675 non-null  float64       \n",
      " 57  sum_amount_90_days                 1296675 non-null  float64       \n",
      " 58  count_amount_90_days               1296675 non-null  float64       \n",
      " 59  avg_amount_90_days                 1296675 non-null  float64       \n",
      " 60  amount_over_average_90_days        1296675 non-null  float64       \n",
      " 61  inter_transaction_time_90_days     1296675 non-null  float64       \n",
      " 62  avg_distance_180_days              1296675 non-null  float64       \n",
      " 63  distance_over_avg_180_days         1296675 non-null  float64       \n",
      " 64  sum_amount_180_days                1296675 non-null  float64       \n",
      " 65  count_amount_180_days              1296675 non-null  float64       \n",
      " 66  avg_amount_180_days                1296675 non-null  float64       \n",
      " 67  amount_over_average_180_days       1296675 non-null  float64       \n",
      " 68  inter_transaction_time_180_days    1296675 non-null  float64       \n",
      " 69  transaction_hour                   1296675 non-null  int64         \n",
      " 70  transaction_day_of_week            1296675 non-null  int64         \n",
      " 71  is_holiday                         1296675 non-null  int64         \n",
      " 72  is_weekend                         1296675 non-null  int64         \n",
      " 73  daily_trans_count                  1296675 non-null  int64         \n",
      " 74  is_high_risk_hour                  1296675 non-null  int64         \n",
      " 75  city_size                          1296675 non-null  int64         \n",
      " 76  merchant_num_trans_1_day_window    1296675 non-null  float64       \n",
      " 77  merchant_risk_1_day_window         1296675 non-null  float64       \n",
      " 78  merchant_num_trans_7_day_window    1296675 non-null  float64       \n",
      " 79  merchant_risk_7_day_window         1296675 non-null  float64       \n",
      " 80  merchant_num_trans_30_day_window   1296675 non-null  float64       \n",
      " 81  merchant_risk_30_day_window        1296675 non-null  float64       \n",
      " 82  merchant_num_trans_90_day_window   1296675 non-null  float64       \n",
      " 83  merchant_risk_90_day_window        1296675 non-null  float64       \n",
      " 84  merchant_num_trans_180_day_window  1296675 non-null  float64       \n",
      " 85  merchant_risk_180_day_window       1296675 non-null  float64       \n",
      "dtypes: datetime64[ns](2), float64(57), int64(15), object(11), uint8(1)\n",
      "memory usage: 842.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_merch_risk.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df_merch_risk.to_csv('../data/processed/train_data.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I later on realized that adding an indicator on if a credict card has been flagged before (within a time window) could be a very good featrue\n",
    "- A ordinal encoding serves as a quick and easy encoding scheme for our categorical features, we could also try customized encoding schemes incorporated with domain knowledge\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraud_detection",
   "language": "python",
   "name": "fraud_detection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
